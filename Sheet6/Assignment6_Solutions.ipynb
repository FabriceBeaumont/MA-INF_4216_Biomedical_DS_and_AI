{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Assignment6_Solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D34dP0oL/4216_Biomedical_DS_and_AI/blob/main/Sheet6/Assignment6_Solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UnHoHsFop-8"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import random as rand"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUswbzmRCHbj"
      },
      "source": [
        "def get_dataset_from_github(filename, index_col_str=None):    \n",
        "    data_file_path = \"https://raw.githubusercontent.com/D34dP0oL/4216_Biomedical_DS_and_AI/main/Datasets/\"\n",
        "    if index_col_str is None:\n",
        "        data = pd.read_csv(data_file_path + filename)\n",
        "    else:\n",
        "        data = pd.read_csv(data_file_path + filename, index_col=index_col_str)\n",
        "\n",
        "    return data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx0G72bGl7xc"
      },
      "source": [
        "## Biomedical Data Science & AI\n",
        "\n",
        "## Assignment 6\n",
        "\n",
        "#### Group members:  Fabrice Beaumont, Fatemeh Salehi, Genivika Mann, Helia Salimi, Jonah"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fhYMe5ml7xo"
      },
      "source": [
        "---\n",
        "### Exercise 1 - NMF clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgUkN-nGziCe"
      },
      "source": [
        "#### 1.1. Write an algorithm to showcase the working of Non-negative matrix factorization (NMF)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appoDszfFI5l"
      },
      "source": [
        "#### 1.2. Mention the pros and cons of NMF as well as one of its applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyw9E17lFKxz"
      },
      "source": [
        "#### 1.3. Use the nimfa package for NMF clustering on the `gene expression data` to cluster genes into groups. Use the parameters (10 ranks, 50 maximum iterations and 25 runs) to compute the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raHIjdHfUEXA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "0e34d8f6-b5f4-4765-9d6f-5a6951dc4a1f"
      },
      "source": [
        "gene_expression_ds = get_dataset_from_github(\"allData.csv\", index_col_str=\"Unnamed: 0\")\n",
        "gene_expression_ds.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>36638_at</th>\n",
              "      <th>39318_at</th>\n",
              "      <th>38514_at</th>\n",
              "      <th>266_s_at</th>\n",
              "      <th>38585_at</th>\n",
              "      <th>41266_at</th>\n",
              "      <th>36108_at</th>\n",
              "      <th>39389_at</th>\n",
              "      <th>31525_s_at</th>\n",
              "      <th>32612_at</th>\n",
              "      <th>36536_at</th>\n",
              "      <th>40202_at</th>\n",
              "      <th>33705_at</th>\n",
              "      <th>33516_at</th>\n",
              "      <th>33412_at</th>\n",
              "      <th>41166_at</th>\n",
              "      <th>32649_at</th>\n",
              "      <th>37006_at</th>\n",
              "      <th>33809_at</th>\n",
              "      <th>1065_at</th>\n",
              "      <th>37701_at</th>\n",
              "      <th>39729_at</th>\n",
              "      <th>40570_at</th>\n",
              "      <th>37043_at</th>\n",
              "      <th>36878_f_at</th>\n",
              "      <th>32035_at</th>\n",
              "      <th>38968_at</th>\n",
              "      <th>307_at</th>\n",
              "      <th>36773_f_at</th>\n",
              "      <th>41165_g_at</th>\n",
              "      <th>296_at</th>\n",
              "      <th>34033_s_at</th>\n",
              "      <th>38242_at</th>\n",
              "      <th>37399_at</th>\n",
              "      <th>41723_s_at</th>\n",
              "      <th>35926_s_at</th>\n",
              "      <th>41164_at</th>\n",
              "      <th>36239_at</th>\n",
              "      <th>34362_at</th>\n",
              "      <th>40775_at</th>\n",
              "      <th>...</th>\n",
              "      <th>35701_at</th>\n",
              "      <th>37888_at</th>\n",
              "      <th>40272_at</th>\n",
              "      <th>1202_g_at</th>\n",
              "      <th>38136_at</th>\n",
              "      <th>38434_at</th>\n",
              "      <th>32327_at</th>\n",
              "      <th>40455_at</th>\n",
              "      <th>1823_g_at</th>\n",
              "      <th>39119_s_at</th>\n",
              "      <th>35828_at</th>\n",
              "      <th>35566_f_at</th>\n",
              "      <th>32180_s_at</th>\n",
              "      <th>40418_at</th>\n",
              "      <th>40262_at</th>\n",
              "      <th>41607_at</th>\n",
              "      <th>394_at</th>\n",
              "      <th>32038_s_at</th>\n",
              "      <th>35677_at</th>\n",
              "      <th>38139_at</th>\n",
              "      <th>32472_at</th>\n",
              "      <th>40077_at</th>\n",
              "      <th>846_s_at</th>\n",
              "      <th>35883_at</th>\n",
              "      <th>31568_at</th>\n",
              "      <th>31880_at</th>\n",
              "      <th>35878_at</th>\n",
              "      <th>34689_at</th>\n",
              "      <th>41060_at</th>\n",
              "      <th>38198_at</th>\n",
              "      <th>37655_at</th>\n",
              "      <th>1440_s_at</th>\n",
              "      <th>32260_at</th>\n",
              "      <th>40070_at</th>\n",
              "      <th>1056_s_at</th>\n",
              "      <th>39200_s_at</th>\n",
              "      <th>36105_at</th>\n",
              "      <th>32578_at</th>\n",
              "      <th>39383_at</th>\n",
              "      <th>33718_at</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>01005</th>\n",
              "      <td>0.583368</td>\n",
              "      <td>0.535258</td>\n",
              "      <td>0.642984</td>\n",
              "      <td>0.891901</td>\n",
              "      <td>0.269871</td>\n",
              "      <td>0.605566</td>\n",
              "      <td>0.069070</td>\n",
              "      <td>0.938101</td>\n",
              "      <td>0.314737</td>\n",
              "      <td>0.590410</td>\n",
              "      <td>0.796561</td>\n",
              "      <td>0.850594</td>\n",
              "      <td>0.918978</td>\n",
              "      <td>0.240408</td>\n",
              "      <td>0.414888</td>\n",
              "      <td>0.691153</td>\n",
              "      <td>0.411294</td>\n",
              "      <td>0.637927</td>\n",
              "      <td>0.594209</td>\n",
              "      <td>0.518397</td>\n",
              "      <td>0.670180</td>\n",
              "      <td>0.332630</td>\n",
              "      <td>0.840712</td>\n",
              "      <td>0.631080</td>\n",
              "      <td>0.473505</td>\n",
              "      <td>0.851113</td>\n",
              "      <td>0.545758</td>\n",
              "      <td>0.457605</td>\n",
              "      <td>0.462939</td>\n",
              "      <td>0.868577</td>\n",
              "      <td>0.330381</td>\n",
              "      <td>0.801885</td>\n",
              "      <td>0.712293</td>\n",
              "      <td>0.134634</td>\n",
              "      <td>0.791588</td>\n",
              "      <td>0.787409</td>\n",
              "      <td>0.991021</td>\n",
              "      <td>0.543582</td>\n",
              "      <td>0.732365</td>\n",
              "      <td>0.176654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.378084</td>\n",
              "      <td>0.483172</td>\n",
              "      <td>0.168982</td>\n",
              "      <td>0.330266</td>\n",
              "      <td>0.419703</td>\n",
              "      <td>0.571903</td>\n",
              "      <td>0.580894</td>\n",
              "      <td>0.234959</td>\n",
              "      <td>0.718483</td>\n",
              "      <td>0.114381</td>\n",
              "      <td>0.267416</td>\n",
              "      <td>0.144547</td>\n",
              "      <td>0.391687</td>\n",
              "      <td>0.348717</td>\n",
              "      <td>0.551471</td>\n",
              "      <td>0.410498</td>\n",
              "      <td>0.364310</td>\n",
              "      <td>0.407837</td>\n",
              "      <td>0.294238</td>\n",
              "      <td>0.237388</td>\n",
              "      <td>0.224433</td>\n",
              "      <td>0.200875</td>\n",
              "      <td>0.318495</td>\n",
              "      <td>0.124299</td>\n",
              "      <td>0.678597</td>\n",
              "      <td>0.353297</td>\n",
              "      <td>0.230118</td>\n",
              "      <td>0.438577</td>\n",
              "      <td>0.464230</td>\n",
              "      <td>0.337547</td>\n",
              "      <td>0.884590</td>\n",
              "      <td>0.399443</td>\n",
              "      <td>0.379796</td>\n",
              "      <td>0.272379</td>\n",
              "      <td>0.216088</td>\n",
              "      <td>0.355740</td>\n",
              "      <td>0.167203</td>\n",
              "      <td>0.438651</td>\n",
              "      <td>0.395720</td>\n",
              "      <td>0.273593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01010</th>\n",
              "      <td>0.505321</td>\n",
              "      <td>0.704177</td>\n",
              "      <td>0.913612</td>\n",
              "      <td>0.657634</td>\n",
              "      <td>0.402911</td>\n",
              "      <td>0.429698</td>\n",
              "      <td>0.803187</td>\n",
              "      <td>0.360471</td>\n",
              "      <td>0.718665</td>\n",
              "      <td>0.773198</td>\n",
              "      <td>0.346023</td>\n",
              "      <td>0.439240</td>\n",
              "      <td>0.575085</td>\n",
              "      <td>0.333801</td>\n",
              "      <td>0.267722</td>\n",
              "      <td>0.717744</td>\n",
              "      <td>0.164642</td>\n",
              "      <td>0.917898</td>\n",
              "      <td>0.095272</td>\n",
              "      <td>0.252275</td>\n",
              "      <td>0.123748</td>\n",
              "      <td>0.465028</td>\n",
              "      <td>0.534974</td>\n",
              "      <td>0.208745</td>\n",
              "      <td>0.748863</td>\n",
              "      <td>0.370238</td>\n",
              "      <td>0.785184</td>\n",
              "      <td>0.561516</td>\n",
              "      <td>0.752276</td>\n",
              "      <td>0.722854</td>\n",
              "      <td>0.224205</td>\n",
              "      <td>0.486049</td>\n",
              "      <td>0.578132</td>\n",
              "      <td>0.073096</td>\n",
              "      <td>0.969421</td>\n",
              "      <td>0.121779</td>\n",
              "      <td>0.639100</td>\n",
              "      <td>0.714173</td>\n",
              "      <td>0.301113</td>\n",
              "      <td>0.218048</td>\n",
              "      <td>...</td>\n",
              "      <td>0.801471</td>\n",
              "      <td>0.399995</td>\n",
              "      <td>0.444195</td>\n",
              "      <td>0.308787</td>\n",
              "      <td>0.128357</td>\n",
              "      <td>0.814556</td>\n",
              "      <td>0.308978</td>\n",
              "      <td>0.276849</td>\n",
              "      <td>0.583248</td>\n",
              "      <td>0.534276</td>\n",
              "      <td>0.540936</td>\n",
              "      <td>0.278203</td>\n",
              "      <td>0.397564</td>\n",
              "      <td>0.209693</td>\n",
              "      <td>0.316325</td>\n",
              "      <td>0.078122</td>\n",
              "      <td>0.366716</td>\n",
              "      <td>0.377046</td>\n",
              "      <td>0.226715</td>\n",
              "      <td>0.237455</td>\n",
              "      <td>0.328624</td>\n",
              "      <td>0.294165</td>\n",
              "      <td>0.441081</td>\n",
              "      <td>0.559321</td>\n",
              "      <td>0.691367</td>\n",
              "      <td>0.297660</td>\n",
              "      <td>0.406114</td>\n",
              "      <td>0.850692</td>\n",
              "      <td>0.225666</td>\n",
              "      <td>0.208741</td>\n",
              "      <td>0.086277</td>\n",
              "      <td>0.432789</td>\n",
              "      <td>0.431440</td>\n",
              "      <td>0.461242</td>\n",
              "      <td>0.443493</td>\n",
              "      <td>0.291054</td>\n",
              "      <td>0.102883</td>\n",
              "      <td>0.584035</td>\n",
              "      <td>0.589994</td>\n",
              "      <td>0.401375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03002</th>\n",
              "      <td>0.375805</td>\n",
              "      <td>0.073716</td>\n",
              "      <td>0.707562</td>\n",
              "      <td>0.847162</td>\n",
              "      <td>0.792428</td>\n",
              "      <td>0.819212</td>\n",
              "      <td>0.644334</td>\n",
              "      <td>0.735292</td>\n",
              "      <td>0.828776</td>\n",
              "      <td>0.821131</td>\n",
              "      <td>0.694593</td>\n",
              "      <td>0.912418</td>\n",
              "      <td>0.925177</td>\n",
              "      <td>0.604762</td>\n",
              "      <td>0.845645</td>\n",
              "      <td>0.102832</td>\n",
              "      <td>0.441828</td>\n",
              "      <td>0.960942</td>\n",
              "      <td>0.822917</td>\n",
              "      <td>0.579433</td>\n",
              "      <td>0.574525</td>\n",
              "      <td>0.209021</td>\n",
              "      <td>0.661056</td>\n",
              "      <td>0.446048</td>\n",
              "      <td>0.711056</td>\n",
              "      <td>0.422506</td>\n",
              "      <td>0.933707</td>\n",
              "      <td>0.548291</td>\n",
              "      <td>0.714249</td>\n",
              "      <td>0.260367</td>\n",
              "      <td>0.282758</td>\n",
              "      <td>0.977896</td>\n",
              "      <td>0.804228</td>\n",
              "      <td>0.809919</td>\n",
              "      <td>0.875175</td>\n",
              "      <td>0.835892</td>\n",
              "      <td>0.395444</td>\n",
              "      <td>0.679245</td>\n",
              "      <td>0.587615</td>\n",
              "      <td>0.268066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333469</td>\n",
              "      <td>0.406121</td>\n",
              "      <td>0.116223</td>\n",
              "      <td>0.227236</td>\n",
              "      <td>0.230363</td>\n",
              "      <td>0.476691</td>\n",
              "      <td>0.638118</td>\n",
              "      <td>0.138736</td>\n",
              "      <td>0.465002</td>\n",
              "      <td>0.177763</td>\n",
              "      <td>0.594686</td>\n",
              "      <td>0.107521</td>\n",
              "      <td>0.554402</td>\n",
              "      <td>0.391740</td>\n",
              "      <td>0.279571</td>\n",
              "      <td>0.297585</td>\n",
              "      <td>0.246278</td>\n",
              "      <td>0.163830</td>\n",
              "      <td>0.349512</td>\n",
              "      <td>0.319535</td>\n",
              "      <td>0.237265</td>\n",
              "      <td>0.570321</td>\n",
              "      <td>0.201130</td>\n",
              "      <td>0.137217</td>\n",
              "      <td>0.659292</td>\n",
              "      <td>0.323094</td>\n",
              "      <td>0.426197</td>\n",
              "      <td>0.176555</td>\n",
              "      <td>0.253732</td>\n",
              "      <td>0.405952</td>\n",
              "      <td>0.750758</td>\n",
              "      <td>0.305592</td>\n",
              "      <td>0.444996</td>\n",
              "      <td>0.293573</td>\n",
              "      <td>0.318939</td>\n",
              "      <td>0.259713</td>\n",
              "      <td>0.271869</td>\n",
              "      <td>0.556874</td>\n",
              "      <td>0.931118</td>\n",
              "      <td>0.301529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>04006</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.226960</td>\n",
              "      <td>0.119596</td>\n",
              "      <td>0.394317</td>\n",
              "      <td>0.115411</td>\n",
              "      <td>0.050117</td>\n",
              "      <td>0.440698</td>\n",
              "      <td>0.649291</td>\n",
              "      <td>0.498758</td>\n",
              "      <td>0.692290</td>\n",
              "      <td>0.087991</td>\n",
              "      <td>0.757562</td>\n",
              "      <td>0.264205</td>\n",
              "      <td>0.178758</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.630364</td>\n",
              "      <td>0.023287</td>\n",
              "      <td>0.051236</td>\n",
              "      <td>0.037160</td>\n",
              "      <td>0.553999</td>\n",
              "      <td>0.181132</td>\n",
              "      <td>0.402491</td>\n",
              "      <td>0.379906</td>\n",
              "      <td>0.139041</td>\n",
              "      <td>0.481713</td>\n",
              "      <td>0.206804</td>\n",
              "      <td>0.526957</td>\n",
              "      <td>0.222823</td>\n",
              "      <td>0.431150</td>\n",
              "      <td>0.722786</td>\n",
              "      <td>0.562790</td>\n",
              "      <td>0.676347</td>\n",
              "      <td>0.385088</td>\n",
              "      <td>0.445281</td>\n",
              "      <td>0.772617</td>\n",
              "      <td>0.686851</td>\n",
              "      <td>0.719016</td>\n",
              "      <td>0.044023</td>\n",
              "      <td>0.266791</td>\n",
              "      <td>0.149898</td>\n",
              "      <td>...</td>\n",
              "      <td>0.820746</td>\n",
              "      <td>0.479706</td>\n",
              "      <td>0.209392</td>\n",
              "      <td>0.283426</td>\n",
              "      <td>0.224231</td>\n",
              "      <td>0.826304</td>\n",
              "      <td>0.036298</td>\n",
              "      <td>0.199957</td>\n",
              "      <td>0.851421</td>\n",
              "      <td>0.203652</td>\n",
              "      <td>0.542015</td>\n",
              "      <td>0.223738</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.210257</td>\n",
              "      <td>0.510002</td>\n",
              "      <td>0.088128</td>\n",
              "      <td>0.628238</td>\n",
              "      <td>0.430658</td>\n",
              "      <td>0.274953</td>\n",
              "      <td>0.062263</td>\n",
              "      <td>0.217099</td>\n",
              "      <td>0.519138</td>\n",
              "      <td>0.394596</td>\n",
              "      <td>0.253829</td>\n",
              "      <td>0.846699</td>\n",
              "      <td>0.030612</td>\n",
              "      <td>0.160331</td>\n",
              "      <td>0.723064</td>\n",
              "      <td>0.393538</td>\n",
              "      <td>0.194421</td>\n",
              "      <td>0.090901</td>\n",
              "      <td>0.365967</td>\n",
              "      <td>0.170796</td>\n",
              "      <td>0.565408</td>\n",
              "      <td>0.456499</td>\n",
              "      <td>0.407845</td>\n",
              "      <td>0.100038</td>\n",
              "      <td>0.670769</td>\n",
              "      <td>0.541601</td>\n",
              "      <td>0.659517</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows Ã— 5000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       36638_at  39318_at  38514_at  ...  32578_at  39383_at  33718_at\n",
              "01005  0.583368  0.535258  0.642984  ...  0.438651  0.395720  0.273593\n",
              "01010  0.505321  0.704177  0.913612  ...  0.584035  0.589994  0.401375\n",
              "03002  0.375805  0.073716  0.707562  ...  0.556874  0.931118  0.301529\n",
              "04006  1.000000  0.226960  0.119596  ...  0.670769  0.541601  0.659517\n",
              "\n",
              "[4 rows x 5000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfmrS-AFOra"
      },
      "source": [
        "#### 1.3.a) From the average connectivity matrix across multiple runs compute consensus matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT5fcXYbFQKJ"
      },
      "source": [
        "#### 1.3.b) Produce a heatmap with a dendrogram from the clustering results you obtained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFoW86HYFRQr"
      },
      "source": [
        "#### 1.3.c) What are the consequences of selecting a rank value that is too small or too large? Implement a method showing how you can optimize the value of the rank to be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NExXYSYPFL78"
      },
      "source": [
        "#### 1.4. Inform yourself about **Non-Negative Matrix Tri-Factorization** (**NMTF**). What is the primary difference between NMF and NMTF and what does it achieve?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnWW-nzRFNGQ"
      },
      "source": [
        "#### 1.5. PCA and NMF are both matrix factorization methods, how do they differ from each other? Describe a situation where PCA is favored over NMF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVP7uDtVTy_H"
      },
      "source": [
        "Description of PCA: ...\n",
        "Description of NMF: ...\n",
        "\n",
        "Differences:\n",
        "- ...\n",
        "\n",
        "Situation where PCA is favored over NMF:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v3MrpiqElLz"
      },
      "source": [
        "---\n",
        "### Exercise 2 - Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsbQAaMbY7q1"
      },
      "source": [
        "#### 2.1. The type of machine learning (e.g. supervised learning, unsupervised learning, etc.) applied depends on the problem at hand. Assume that we have an `Alzheimer's disease (AD) dataset` where rows represent 500 participants and columns represent 100 different collected measurements for each participant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjUHF7RUE3Cp"
      },
      "source": [
        "##### 2.1.a) You are asked to train a model that can predict whether a participant is healthy or AD. Mention the type of machine learning you would use for this case scenario and elaborate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XRqVmMXXV0T"
      },
      "source": [
        "We would use supervised learning because in this dataset we are dealing with labeled samples that we can use to learn how to correctly classify unseen participants. We have a very small amount of data at hand and so it is important that we choose a low-complexity model to not overfit the given data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDPR_PU5E6M5"
      },
      "source": [
        "##### 2.1.b) Assume that we do not have any information about the diagnosis of each participant. This time we would like to divide our participants into groups based on the features that we have in hand. What type of machine learning would be appropriate for this scenario and elaborate?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcpiLOTBbK0X"
      },
      "source": [
        "In this case we can't use supervised learning as our samples are not labeled, so we would choose unsupervised learning. Our goal then is to learn the inherent structure of our samples and find clusters that could then be used to categorize new samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIcUlAxgE7Wd"
      },
      "source": [
        "##### 2.1.c) Imagine that the shape of our dataset is *(100, 600)*, mention one pre-processing step that you would take to carry out the tasks *2.1.a)* and *2.1.b)*?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-US2V4IRdxVx"
      },
      "source": [
        "2.1.a) We would slice the dataset into two distinct sets, namely a training and a test set. The training set is used to train the supervised learning algorithm we're using. The test set is then used to evaluate the model and is used once the model is trained. We would split the dataset at 500 participants so that we have a training set of size (100, 500) and a test set of size (100, 100).\n",
        "\n",
        "2.1.b) For unsupervised learning in this case we do not need to split the set into training and test set as we want to find the inherent structure of the given data and so it is crucial to have as much data at hand as possible. Also we wouldn't be able to evaluate the model with a test set because there is no way to evaluate the performance without given labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Phw_HR48Ey9z"
      },
      "source": [
        "#### 2.2. Generate a pipeline in scikit learn using the following code snippet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4jSTy2jG6FZ"
      },
      "source": [
        "####################### Given Code Snippet ############################\n",
        "# polynomial_features = PolynomialFeatures(degree=15, include_bias=False)\n",
        "# linear_regression = LinearRegression()\n",
        "\n",
        "# pipeline = Pipeline([\n",
        "#     (\"polynomial_features\", polynomial_features),\n",
        "#     (\"linear_regression\", linear_regression)\n",
        "# ])`\n",
        "#######################################################################\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "polynomial_features = PolynomialFeatures(degree=15, include_bias=False)\n",
        "linear_regression = LinearRegression()\n",
        "\n",
        "pipeline = Pipeline([\n",
        "  (\"polynomial_features\", polynomial_features),\n",
        "  (\"linear_regression\", linear_regression)\n",
        "])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QTFGk1zHUv9"
      },
      "source": [
        "##### 2.2.a) Using the Fish dataset provided, identify the quality of fit of the pipeline for the dataset (use the weight as the response variable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioSQB6QQpIQR",
        "outputId": "3b272c01-c2f9-424e-babb-6ae849ad6ff1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "fish_ds = get_dataset_from_github(\"Fish.csv\")\n",
        "fish_ds.head(4)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Species</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Length1</th>\n",
              "      <th>Length2</th>\n",
              "      <th>Length3</th>\n",
              "      <th>Height</th>\n",
              "      <th>Width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bream</td>\n",
              "      <td>242.0</td>\n",
              "      <td>23.2</td>\n",
              "      <td>25.4</td>\n",
              "      <td>30.0</td>\n",
              "      <td>11.5200</td>\n",
              "      <td>4.0200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bream</td>\n",
              "      <td>290.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>26.3</td>\n",
              "      <td>31.2</td>\n",
              "      <td>12.4800</td>\n",
              "      <td>4.3056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bream</td>\n",
              "      <td>340.0</td>\n",
              "      <td>23.9</td>\n",
              "      <td>26.5</td>\n",
              "      <td>31.1</td>\n",
              "      <td>12.3778</td>\n",
              "      <td>4.6961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bream</td>\n",
              "      <td>363.0</td>\n",
              "      <td>26.3</td>\n",
              "      <td>29.0</td>\n",
              "      <td>33.5</td>\n",
              "      <td>12.7300</td>\n",
              "      <td>4.4555</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
              "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
              "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
              "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
              "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHo1tWUTqB76",
        "outputId": "cd9e9cb6-3fe9-42bb-d110-38608c0bbf9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = fish_ds['Weight'].to_numpy()\n",
        "X = fish_ds.drop(['Weight'], axis=1).to_numpy()\n",
        "# replace the species strings with numbers. Should we do this? seems wrong to me as its categorical\n",
        "_, X[:,0] = np.unique(X[:,0], return_inverse=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "pipeline.score(X_test, y_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-47656136990019.78"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfsAnU8HaY9"
      },
      "source": [
        "##### 2.2.b) If the pipeline produces a badly fit model for the dataset, list some methods to improve the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FT2Xl_AE1Fk"
      },
      "source": [
        "#### 2.3. In this exercise we will compare the accuracy of different methods on a high-dimensional ($p>>n$) dataset. Load the leukemia_small.csv and extract the class labels from the column names (2 classes, `AML` and `ALL`).Randomly split the data into *70%* training and *30%* test.\n",
        "\n",
        "**Hint:** Use the `train_test_split` function from scikit-learn to define the test_size and\n",
        "set `random_state=1` for better reproducibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W8jwBlpE9z7"
      },
      "source": [
        "##### 2.3.a) Fit a logistic regression (no penalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg4ZR1dOE_gU"
      },
      "source": [
        "##### 2.3.b) Fit multiple $l_1$-penalized logistic regressions ($\\lambda \\in \\{ 0.001,\\ 0.01,\\  0.1,\\ \n",
        "1,\\  10,\\ 100\\}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33sv9n9aFApt"
      },
      "source": [
        "##### 2.3.c) Fit multiple $l_2$-penalized logistic regressions ($\\lambda \\in \\{ 0.001,\\ 0.01,\\  0.1,\\ \n",
        "1,\\  10,\\ 100\\}$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1tnymZFBbW"
      },
      "source": [
        "##### 2.3.d) For the models from *2.3.a)*, *2.3.b)*, and *2.3.c)* measure the performance on the training and test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G_aB_cVFCbN"
      },
      "source": [
        "##### 2.3.e) Using *2.3.d)* report the performances with one scatterplot for each approach (one scatterplot for each: unpenalized, $l_1$, $l_2$), with the regularization constant on the $x$-axis and the accuracy on the $y$-axis, train and test set colored differently, proper axis labels and a legend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UswnbGnvFDY5"
      },
      "source": [
        "##### 2.3.f) Which method in combination with which parameter gives the best results on the test set?"
      ]
    }
  ]
}